#Hadoop MapReduce - Group 26

# First of all, we created EC2 instances (master, slave1, slave2) from an existing AMI.

1) sudo nano /etc/hosts
172.31.27.188 master
172.31.28.147 slave1
172.31.29.250 slave2

# Upon connecting to our instance, we switched our user from root to Hadoop.
2)  sudo su - hadoop

# Next, we ping both slave1 and slave2 to test whether all machines are set.
3) ping slave1 and slave2

# We then initialized the HDFS namenode, preparing it for use.
4) hdfs namenode -format

# We started all the HDFS services, including the namenode, datanode, jobtracker, and tasktracker.
5) start-all.sh

# We checked whether these folders exist in the HDFS. 
6) hadoop fs -mkdir /user
7) hadoop fs -mkdir /user/hadoop

# We changed the current directory to IST3134.
8) cd IST3134/

# We extracted the contents of the wordcount.zip file.
9) unzip wordcount.zip

# We created a workspace directory in our home directory.
10) mkdir ~/workspace

# We copied the wordcount directory to the workspace directory.
11) cp -r wordcount/ ~/workspace/

# We changed the current directory to our home directory.
12) cd ~

# We downloaded the dataset file from the AWS S3 Bucket into the Hadoop directory.
13) wget "https://basketbucketbelle.s3.amazonaws.com/precovid_reviews_subset_55.csv"

# We uploaded the dataset file to the Hadoop directory.
14) hadoop fs -put precovid_reviews_subset_55.csv /user/hadoop/

# We changed the current directory to 
15) cd ~/workspace/wordcount/src

# We checked the contents of the stubs package directory.
16) ls stubs

# We examined the classpath Hadoop is configured to use.
17) hadoop classpath

# Next, we compiled the three Java paths.
18) javac -classpath `hadoop classpath` stubs/*.java

# We then collected the compiled Java files and put them into a JAR file.
19) jar cvf wc.jar stubs/*.class

# To count the number of appearances of each word in the dataset, we submitted a MapReduce job to Hadoop using the JAR file.
20) hadoop jar wc.jar stubs.WordCount precovid_reviews_subset_55.csv wordcounts1

# We ran this code to show the word count results.
21) hadoop fs -ls wordcounts1

# To show the first 70 rows and sort the word count results in descending order, we ran this code so that we could identify which words has the highest frequency.
22) hadoop fs -cat /user/hadoop/wordcounts1/part-r-00000 | sort -nr -k2 | head -n 70







